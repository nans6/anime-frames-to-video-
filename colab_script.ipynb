{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip -q attack-on-genai.zip\n",
        "%cd attack-on-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd eval\n",
        "!git clone https://github.com/JunyaoHu/common_metrics_on_video_quality\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hWEz-Y67qRp"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Login to Hugging Face\n",
        "# Option 1: Use token from environment variable (recommended for Colab)\n",
        "# Set HF_TOKEN in Colab secrets or environment variables\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in to Hugging Face using token from environment variable\")\n",
        "else:\n",
        "    # Option 2: Interactive login (will prompt for token)\n",
        "    print(\"Please enter your Hugging Face token:\")\n",
        "    login()\n",
        "    print(\"Logged in to Hugging Face\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "wandb_project = \"attack-on-genai\"\n",
        "wandb_entity = \"attack-on-genai\"\n",
        "wandb_run_name = os.getenv(\"WANDB_RUN_NAME\") or f\"wan-train-{datetime.datetime.now():%Y%m%d-%H%M%S}\"\n",
        "\n",
        "if wandb_api_key:\n",
        "    wandb.login(key=wandb_api_key)\n",
        "    print(\"Logged in to Weights & Biases using WANDB_API_KEY\")\n",
        "else:\n",
        "    print(\"WANDB_API_KEY not set; you'll be prompted to log in.\")\n",
        "    wandb.login()\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
        "os.environ[\"WANDB_RUN_NAME\"] = wandb_run_name\n",
        "os.environ.setdefault(\"WANDB_MODE\", \"online\")\n",
        "os.environ[\"WANDB_ENTITY\"] = wandb_entity\n",
        "\n",
        "print(f\"W&B project: {wandb_project}\")\n",
        "print(f\"W&B run: {wandb_run_name}\")\n",
        "print(f\"W&B entity: {wandb_entity}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SETUP: Download & Split Dataset\n",
        "dataset_repo_id = \"attack-on-genai/video-frames\"\n",
        "checkpoint_dir = \"checkpoint_setup\"\n",
        "max_samples = 5\n",
        "test_split = 0.2\n",
        "seed = 42\n",
        "\n",
        "setup_cmd = f\"\"\"python train_setup.py \\\n",
        "    --dataset_repo_id {dataset_repo_id} \\\n",
        "    --checkpoint_dir {checkpoint_dir} \\\n",
        "    --max_samples {max_samples} \\\n",
        "    --test_split {test_split} \\\n",
        "    --seed {seed}\"\"\"\n",
        "\n",
        "!{setup_cmd}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hLjwQ1S7uSy"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "train_data_dir = \"checkpoint_setup/train\"  \n",
        "dataset_repo_id = None  \n",
        "model_id = \"Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers\"\n",
        "output_dir = \"wan_flf2v_lora\"\n",
        "\n",
        "max_train_steps = 50\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-2\n",
        "batch_size = 1\n",
        "gradient_accumulation_steps = 1\n",
        "log_every = 1\n",
        "save_every = 1000\n",
        "max_samples = None  \n",
        "\n",
        "lora_rank = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.0\n",
        "\n",
        "dynamic_frames = True  \n",
        "num_frames = 17  \n",
        "flow_shift = 3.0\n",
        "quantization = \"no\"\n",
        "mixed_precision = \"bf16\"  \n",
        "device = \"cuda\"\n",
        "\n",
        "# Optical flow loss (optional but improves motion quality)\n",
        "use_flow_loss = True  \n",
        "flow_loss_weight = 0.2  \n",
        "flow_downsample = 1  \n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(\n",
        "    f\"Logging to W&B project={os.getenv('WANDB_PROJECT', 'attack-on-genai')} \"\n",
        "    f\"run={os.getenv('WANDB_RUN_NAME', 'wan-train-manual')} \"\n",
        "    f\"entity={os.getenv('WANDB_ENTITY', 'attack-on-genai')}\"\n",
        ")\n",
        "\n",
        "cmd_parts = [\n",
        "    \"python train.py\",\n",
        "    f\"--train_data_dir {train_data_dir}\" if train_data_dir else None,\n",
        "    f\"--dataset_repo_id {dataset_repo_id}\" if dataset_repo_id else None,\n",
        "    f\"--model_id {model_id}\",\n",
        "    f\"--output_dir {output_dir}\",\n",
        "    f\"--max_train_steps {max_train_steps}\",\n",
        "    f\"--learning_rate {learning_rate}\",\n",
        "    f\"--weight_decay {weight_decay}\",\n",
        "    f\"--batch_size {batch_size}\",\n",
        "    f\"--gradient_accumulation_steps {gradient_accumulation_steps}\",\n",
        "    f\"--log_every {log_every}\",\n",
        "    f\"--save_every {save_every}\",\n",
        "    f\"--lora_rank {lora_rank}\",\n",
        "    f\"--lora_alpha {lora_alpha}\",\n",
        "    f\"--lora_dropout {lora_dropout}\",\n",
        "    \"--dynamic_frames\" if dynamic_frames else None,\n",
        "    f\"--num_frames {num_frames}\" if not dynamic_frames else None,\n",
        "    f\"--flow_shift {flow_shift}\",\n",
        "    f\"--max_samples {max_samples}\" if max_samples else None,\n",
        "    f\"--mixed_precision {mixed_precision}\",\n",
        "    f\"--quantization {quantization}\",\n",
        "    f\"--device {device}\",\n",
        "    \"--use_flow_loss\" if use_flow_loss else None,\n",
        "    f\"--flow_loss_weight {flow_loss_weight}\" if use_flow_loss else None,\n",
        "    f\"--flow_downsample {flow_downsample}\" if use_flow_loss else None,\n",
        "]\n",
        "cmd = \" \\\\\\n    \".join([p for p in cmd_parts if p])\n",
        "\n",
        "print(\"Running:\\n\", cmd)\n",
        "!{cmd}\n",
        "\n",
        "print(f\"Training complete. W&B run configured via environment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPLOAD TO HUGGINGFACE (Optional) \n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "output_dir = \"wan_flf2v_lora\"\n",
        "repo_id = \"attack-on-genai/wan-finetune\"\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    hf_token = os.getenv(\"HF_TOKEN\")\n",
        "    api = HfApi(token=hf_token)\n",
        "    api.upload_folder(\n",
        "        folder_path=output_dir,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        commit_message=\"Upload LoRA weights\"\n",
        "    )\n",
        "    print(f\"Uploaded to https://huggingface.co/{repo_id}\")\n",
        "else:\n",
        "    print(f\"{output_dir} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUATION\n",
        "import os\n",
        "import json\n",
        "\n",
        "hf_lora_repo = \"attack-on-genai/wan-finetune\"  \n",
        "test_data_dir = \"checkpoint_setup/test\"\n",
        "resize_for_eval = 480  \n",
        "output_json = \"evaluation_results.json\"\n",
        "device = \"cuda\"\n",
        "\n",
        "eval_frame_counts = \"13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77,81\" \n",
        "\n",
        "num_inference_steps = 30\n",
        "eval_base = True  \n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "eval_cmd = f\"\"\"python evaluation.py \\\n",
        "    --hf_lora_repo {hf_lora_repo} \\\n",
        "    --test_data_dir {test_data_dir} \\\n",
        "    --eval_frame_counts {eval_frame_counts} \\\n",
        "    --num_inference_steps {num_inference_steps} \\\n",
        "    --resize {resize_for_eval} \\\n",
        "    --output_json {output_json} \\\n",
        "    --device {device} \\\n",
        "    {\"--eval_base\" if eval_base else \"\"}\"\"\"\n",
        "\n",
        "print(f\"Evaluating on frame counts: {eval_frame_counts}\")\n",
        "!{eval_cmd}\n",
        "\n",
        "if os.path.exists(output_json):\n",
        "    with open(output_json, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(\"\\nEvaluation Results by Frame Count:\")\n",
        "    for frame_key in sorted(results.keys()):\n",
        "        if frame_key.startswith('frames_'):\n",
        "            frame_count = frame_key.split('_')[1]\n",
        "            print(f\"\\n{frame_key} ({frame_count} frames):\")\n",
        "            if 'finetuned' in results[frame_key]:\n",
        "                for m in ['fvd', 'ssim', 'psnr', 'lpips']:\n",
        "                    if m in results[frame_key]['finetuned']:\n",
        "                        print(f\"  {m.upper()}: {results[frame_key]['finetuned'][m]['value'][0]:.4f}\")\n",
        "else:\n",
        "    print(f\"{output_json} not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BATCH INFERENCE: 3 BASE + 3 FINETUNED (examples)\n",
        "import os\n",
        "import subprocess\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "examples_dir = \"examples\"\n",
        "hf_lora_repo = \"attack-on-genai/wan-finetune\"\n",
        "prompt = \"high quality anime style, smooth motion, consistent characters\"\n",
        "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
        "num_frames = 13\n",
        "num_inference_steps = 30\n",
        "seed = 42\n",
        "\n",
        "example_pairs = [\n",
        "    (\"first_1.png\", \"last_1.png\", \"ex1\"),\n",
        "    (\"first_2.png\", \"last_2.png\", \"ex2\"),\n",
        "    (\"first_3.png\", \"last_3.png\", \"ex3\"),\n",
        "]\n",
        "\n",
        "wandb_project = os.getenv(\"WANDB_PROJECT\", \"attack-on-genai\")\n",
        "wandb_entity = os.getenv(\"WANDB_ENTITY\") or None\n",
        "wandb_run_name = os.getenv(\"WANDB_RUN_NAME\") or \"wan-multi-infer\"\n",
        "wandb_run_id = os.getenv(\"WANDB_RUN_ID\")\n",
        "\n",
        "if wandb_run_id:\n",
        "    wb = wandb.init(project=wandb_project, entity=wandb_entity, id=wandb_run_id, resume=\"must\")\n",
        "else:\n",
        "    wb = wandb.init(project=wandb_project, entity=wandb_entity, name=wandb_run_name)\n",
        "\n",
        "logged_videos = {}\n",
        "start_time = time.time()\n",
        "total_videos = len(example_pairs) * 2  \n",
        "\n",
        "with tqdm(total=total_videos, desc=\"Generating examples\", unit=\"video\") as pbar:\n",
        "    for first_name, last_name, tag in example_pairs:\n",
        "        first_path = os.path.join(examples_dir, first_name)\n",
        "        last_path = os.path.join(examples_dir, last_name)\n",
        "        assert os.path.exists(first_path), f\"Missing {first_path}\"\n",
        "        assert os.path.exists(last_path), f\"Missing {last_path}\"\n",
        "\n",
        "        base_out = f\"{tag}_base.mp4\"\n",
        "        ft_out = f\"{tag}_finetuned.mp4\"\n",
        "\n",
        "        base_cmd = [\n",
        "            \"python\", \"inference.py\",\n",
        "            \"--first_frame\", first_path,\n",
        "            \"--last_frame\", last_path,\n",
        "            \"--output\", base_out,\n",
        "            \"--prompt\", prompt,\n",
        "            \"--negative_prompt\", negative_prompt,\n",
        "            \"--num_frames\", str(num_frames),\n",
        "            \"--num_inference_steps\", str(num_inference_steps),\n",
        "            \"--seed\", str(seed),\n",
        "        ]\n",
        "        pbar.set_description(f\"Generating {tag} (base)\")\n",
        "        subprocess.run(base_cmd, check=True)\n",
        "        pbar.update(1)\n",
        "\n",
        "        ft_cmd = [\n",
        "            \"python\", \"inference.py\",\n",
        "            \"--lora_path\", hf_lora_repo,\n",
        "            \"--first_frame\", first_path,\n",
        "            \"--last_frame\", last_path,\n",
        "            \"--output\", ft_out,\n",
        "            \"--prompt\", prompt,\n",
        "            \"--negative_prompt\", negative_prompt,\n",
        "            \"--num_frames\", str(num_frames),\n",
        "            \"--num_inference_steps\", str(num_inference_steps),\n",
        "            \"--seed\", str(seed),\n",
        "        ]\n",
        "        pbar.set_description(f\"Generating {tag} (finetuned)\")\n",
        "        subprocess.run(ft_cmd, check=True)\n",
        "        pbar.update(1)\n",
        "\n",
        "        wb.log({\n",
        "            f\"examples/{tag}/base\": wandb.Video(base_out, fps=16, format=\"mp4\"),\n",
        "            f\"examples/{tag}/finetuned\": wandb.Video(ft_out, fps=16, format=\"mp4\"),\n",
        "        })\n",
        "\n",
        "print(\"Logged 3 base + 3 finetuned videos to W&B\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEdJLbuW7038"
      },
      "outputs": [],
      "source": [
        "# Inference configuration\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "hf_lora_repo = \"attack-on-genai/wan-finetune\"\n",
        "first_frame_path = \"first_frame.png\"\n",
        "last_frame_path = \"last_frame.png\"\n",
        "prompt = \"high quality anime style, smooth motion, consistent characters\"\n",
        "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
        "num_frames = 17\n",
        "num_inference_steps = 30\n",
        "seed = 42\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with tqdm(total=2, desc=\"Generating inference examples\", unit=\"video\") as pbar:\n",
        "    # BASE MODEL\n",
        "    pbar.set_description(\"Generating BASE MODEL (no LoRA)\")\n",
        "    base_cmd = f\"\"\"python inference.py \\\n",
        "        --first_frame {first_frame_path} \\\n",
        "        --last_frame {last_frame_path} \\\n",
        "        --output base_output.mp4 \\\n",
        "        --prompt \"{prompt}\" \\\n",
        "        --negative_prompt \"{negative_prompt}\" \\\n",
        "        --num_frames {num_frames} \\\n",
        "        --num_inference_steps {num_inference_steps} \\\n",
        "        --seed {seed if seed is not None else 'None'}\"\"\"\n",
        "    !{base_cmd}\n",
        "    pbar.update(1)\n",
        "\n",
        "    # FINE-TUNED MODEL\n",
        "    pbar.set_description(\"Generating FINE-TUNED MODEL (with LoRA)\")\n",
        "    ft_cmd = f\"\"\"python inference.py \\\n",
        "        --lora_path {hf_lora_repo} \\\n",
        "        --first_frame {first_frame_path} \\\n",
        "        --last_frame {last_frame_path} \\\n",
        "        --output finetuned_output.mp4 \\\n",
        "        --prompt \"{prompt}\" \\\n",
        "        --negative_prompt \"{negative_prompt}\" \\\n",
        "        --num_frames {num_frames} \\\n",
        "        --num_inference_steps {num_inference_steps} \\\n",
        "        --seed {seed if seed is not None else 'None'}\"\"\"\n",
        "    !{ft_cmd}\n",
        "    pbar.update(1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ“ Done! Generated videos:\")\n",
        "print(\"  - base_output.mp4 (base model)\")\n",
        "print(\"  - finetuned_output.mp4 (fine-tuned model)\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
